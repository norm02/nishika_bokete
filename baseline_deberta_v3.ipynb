{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ボケ判定AIを作ろう！-チュートリアル1\n",
        "このnotebookは、Nishikaコンペティション [ボケ判定AIを作ろう！](https://www.nishika.com/competitions/) のチュートリアルです。\n",
        "\n",
        "「ボケて」データを用いて、画像データと文章からそのボケてが面白いか面白くないかを予測することをテーマとしています。\n",
        "\n",
        "このNotebookでは、画像とテキストそれぞれの特徴量生成を以下のような方法で行っていきます。\n",
        "\n",
        "- CNNモデルを用いた画像データの特徴量化\n",
        "- BERTモデルを用いたテキストデータの特徴量化\n",
        "\n",
        "特徴量の作成では、テキストと画像それぞれ別々で作成していますので、画像データとテキストデータを組み合わせた特徴量を入れることで精度向上が見込めるかも知れませんので、いろいろと試していただければと思います。"
      ],
      "metadata": {
        "id": "m0ayzrjm-DgE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| 要素 | 説明 |\n",
        "| ---- | ---- |\n",
        "|id | ID|\n",
        "|odai_photo_file_name | ボケてのお題画像|\n",
        "|text | ボケての文章|\n",
        "|is_laugh | 面白さ（面白い：１、面白くない：０）|\n"
      ],
      "metadata": {
        "id": "EjvjywgsCuKU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ディレクトリ構成は以下のように設定します\n",
        "\n",
        "```\n",
        "├── train.zip\n",
        "│ ├── xxx.jpg\n",
        "│ └── yyy.jpg\n",
        "├── test.zip\n",
        "│ ├── xxx.jpg\n",
        "│ └── yyy.jpg\n",
        "├── train.csv\n",
        "├── test.csv\n",
        "├── sample_submission.csv\n",
        "└── submission.csv(今回のbaselineで生成されるsubmissionファイル)\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "### setting\n",
        "ページ上部の「ランタイム」>「ランタイムのタイプを変更」から「GPU」「ハイメモリ」を選択"
      ],
      "metadata": {
        "id": "9YJVABJ8CqoX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exPnU6gJ9_8L",
        "outputId": "697116f2-8742-4d9f-a3dc-b26cd40dcbec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9onj0RDDiWU4",
        "outputId": "775d7f64-07e6-4acd-88e3-a820d4ada770"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Sep 25 06:24:17 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P0    29W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Library"
      ],
      "metadata": {
        "id": "JGai1EPR_tEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install deberta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4MOV1LaTSCHi",
        "outputId": "2f8b4a7a-32a5-4388-bcd6-ce52efa36fc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting deberta\n",
            "  Downloading DeBERTa-0.1.12-py3-none-any.whl (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 2.0 MB/s \n",
            "\u001b[?25hCollecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from deberta) (1.12.1+cu113)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from deberta) (2022.6.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from deberta) (1.7.3)\n",
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from deberta) (4.64.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from deberta) (3.7)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 8.4 MB/s \n",
            "\u001b[?25hCollecting laser\n",
            "  Downloading LASER-0.0.5-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: ujson in /usr/local/lib/python3.7/dist-packages (from deberta) (5.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from deberta) (1.21.6)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from deberta) (5.4.8)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (from deberta) (3.4.1)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from deberta) (3.6.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->deberta) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->deberta) (7.1.2)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->deberta) (1.4.1)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->deberta) (8.14.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->deberta) (0.7.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pytest->deberta) (57.4.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from pytest->deberta) (1.15.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->deberta) (1.11.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->deberta) (22.1.0)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval->deberta) (1.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval->deberta) (3.1.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy->deberta) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy->deberta) (4.1.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy->deberta) (2.11.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy->deberta) (3.0.10)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy->deberta) (2.23.0)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy->deberta) (8.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy->deberta) (21.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->deberta) (3.0.7)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy->deberta) (0.4.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy->deberta) (1.9.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy->deberta) (2.0.8)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy->deberta) (1.0.8)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy->deberta) (1.0.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->deberta) (2.0.6)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy->deberta) (2.4.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy->deberta) (0.10.1)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy->deberta) (0.6.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy->deberta) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy->deberta) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy->deberta) (5.2.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->deberta) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->deberta) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->deberta) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->deberta) (1.24.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy->deberta) (0.7.8)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy->deberta) (2.0.1)\n",
            "Building wheels for collected packages: seqeval, sklearn\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16180 sha256=240719059f74a8d286be475aca9ac0d6fc849e22a355c3a9dbd05c916ef5a2d5\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1310 sha256=2ae077aeb2e3f6eca587caf76bb75ec69e816916bbe7cf6896eb36df30fbd11c\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/ef/c3/157e41f5ee1372d1be90b09f74f82b10e391eaacca8f22d33e\n",
            "Successfully built seqeval sklearn\n",
            "Installing collected packages: sklearn, seqeval, sentencepiece, laser, deberta\n",
            "Successfully installed deberta-0.1.12 laser-0.0.5 sentencepiece-0.1.97 seqeval-1.2.2 sklearn-0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git lfs install\n",
        "!git clone https://huggingface.co/microsoft/deberta-v3-large"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCMrhTaIO2ih",
        "outputId": "a3214526-9fcf-4abb-c8aa-8b6c4ca4ecc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Failed to call git rev-parse --git-dir --show-toplevel: \"fatal: not a git repository (or any of the parent directories): .git\\n\"\n",
            "Git LFS initialized.\n",
            "Cloning into 'deberta-v3-large'...\n",
            "remote: Enumerating objects: 68, done.\u001b[K\n",
            "remote: Total 68 (delta 0), reused 0 (delta 0), pack-reused 68\u001b[K\n",
            "Unpacking objects: 100% (68/68), done.\n",
            "tcmalloc: large alloc 1471086592 bytes == 0x556908a62000 @  0x7f6bcdae02a4 0x5568ccf9511f 0x5568ccf7225b 0x5568ccf26f33 0x5568ccecb22a 0x5568ccecb6e6 0x5568ccee8451 0x5568ccee89e9 0x5568ccee8f13 0x5568ccf8de12 0x5568cce2f162 0x5568cce15a65 0x5568cce16725 0x5568cce1572a 0x7f6bcce27c87 0x5568cce1577a\n",
            "tcmalloc: large alloc 2206621696 bytes == 0x556960552000 @  0x7f6bcdae02a4 0x5568ccf9511f 0x5568ccf7225b 0x5568ccf26f33 0x5568ccecb22a 0x5568ccecb6e6 0x5568ccee8451 0x5568ccee89e9 0x5568ccee8f13 0x5568ccf8de12 0x5568cce2f162 0x5568cce15a65 0x5568cce16725 0x5568cce1572a 0x7f6bcce27c87 0x5568cce1577a\n",
            "Filtering content: 100% (3/3), 2.43 GiB | 78.04 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install tokenizers\n",
        "!pip install --quiet sentencepiece\n",
        "!pip install --quiet japanize-matplotlib\n",
        "!pip install transformers fugashi ipadic >> /dev/null"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23tLb36S_nX-",
        "outputId": "6afef21a-cac9-4f7e-abab-6db5e92cfec8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.18.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.9.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.53)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.7/dist-packages (0.12.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "import japanize_matplotlib\n",
        "\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import BertTokenizer, BertJapaneseTokenizer\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.model_selection import StratifiedKFold, KFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import re\n",
        "import random\n",
        "\n",
        "from time import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "from contextlib import contextmanager\n",
        "import lightgbm as lgb\n",
        "\n",
        "import re\n",
        "import requests\n",
        "import unicodedata\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from bs4 import BeautifulSoup\n",
        "nltk.download(['wordnet', 'stopwords', 'punkt'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Myu_A_Kc_zyK",
        "outputId": "961404d2-986c-4050-8513-97b4ba8ff5d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting"
      ],
      "metadata": {
        "id": "dCYVrA1lAB8L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_everything(seed=1234):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "\n",
        "seed_everything(42)"
      ],
      "metadata": {
        "id": "z9WsZjx-NT-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT = \"/content/drive/MyDrive/nishika\" # 所望のディレクトリに変更してください。\n",
        "train_image_path = \"/content/drive/MyDrive/nishika/train/\"\n",
        "test_image_path = \"/content/drive/MyDrive/nishika/test/\""
      ],
      "metadata": {
        "id": "ajiBxfC2ABC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read Data\n",
        "学習データと推論データについて、目的変数の分布などを確認していきます。"
      ],
      "metadata": {
        "id": "qbdARhsQASHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(os.path.join(INPUT, \"train.csv\"))\n",
        "test_df = pd.read_csv(os.path.join(INPUT, \"test.csv\"))\n",
        "submission_df = pd.read_csv(os.path.join(INPUT, \"sample_submission.csv\"))"
      ],
      "metadata": {
        "id": "Nh06H3rFAPA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"train_data: {train_df.shape}\")\n",
        "display(train_df.head())\n",
        "\n",
        "print(f\"test_data: {test_df.shape}\")\n",
        "display(test_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "sofMAgYkAqmU",
        "outputId": "24ad2dc3-7528-43d8-cda0-57d37fe6d2df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_data: (24962, 4)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "          id odai_photo_file_name  \\\n",
              "0  ge5kssftl       9fkys1gb2r.jpg   \n",
              "1  r7sm6tvkj       c6ag0m1lak.jpg   \n",
              "2  yp5aze0bh       whtn6gb9ww.jpg   \n",
              "3  ujaixzo56       6yk5cwmrsy.jpg   \n",
              "4  7vkeveptl       0i9gsa2jsm.jpg   \n",
              "\n",
              "                                            text  is_laugh  \n",
              "0             君しょっちゅうソレ自慢するけど、ツムジ２個ってそんなに嬉しいのかい？         0  \n",
              "1                            これでバレない？授業中寝てもバレない？         0  \n",
              "2  「あなたも感じる？」\\n『ああ…、感じてる…』\\n「後ろに幽霊いるよね…」\\n『女のな…』         0  \n",
              "3                 大塚愛聞いてたらお腹減った…さく、らんぼと牛タン食べたい…          0  \n",
              "4                                    熊だと思ったら嫁だった         0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f290ea2d-ece0-461d-a63d-cf7d072e993a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>odai_photo_file_name</th>\n",
              "      <th>text</th>\n",
              "      <th>is_laugh</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ge5kssftl</td>\n",
              "      <td>9fkys1gb2r.jpg</td>\n",
              "      <td>君しょっちゅうソレ自慢するけど、ツムジ２個ってそんなに嬉しいのかい？</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>r7sm6tvkj</td>\n",
              "      <td>c6ag0m1lak.jpg</td>\n",
              "      <td>これでバレない？授業中寝てもバレない？</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>yp5aze0bh</td>\n",
              "      <td>whtn6gb9ww.jpg</td>\n",
              "      <td>「あなたも感じる？」\\n『ああ…、感じてる…』\\n「後ろに幽霊いるよね…」\\n『女のな…』</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ujaixzo56</td>\n",
              "      <td>6yk5cwmrsy.jpg</td>\n",
              "      <td>大塚愛聞いてたらお腹減った…さく、らんぼと牛タン食べたい…</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7vkeveptl</td>\n",
              "      <td>0i9gsa2jsm.jpg</td>\n",
              "      <td>熊だと思ったら嫁だった</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f290ea2d-ece0-461d-a63d-cf7d072e993a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f290ea2d-ece0-461d-a63d-cf7d072e993a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f290ea2d-ece0-461d-a63d-cf7d072e993a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_data: (6000, 3)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "          id odai_photo_file_name  \\\n",
              "0  rfdjcfsqq       nc1kez326b.jpg   \n",
              "1  tsgqmfpef       49xt2fmjw0.jpg   \n",
              "2  owjcthkz2       9dtscjmyfh.jpg   \n",
              "3  rvgaocjyy       osa3n56tiv.jpg   \n",
              "4  uxtwu5i69       yb1yqs4pvb.jpg   \n",
              "\n",
              "                                                text  \n",
              "0                          僕のママ、キャラ弁のゆでたまごに８時間かかったんだ  \n",
              "1                                          かわいいが作れた！  \n",
              "2                                           来世の志茂田景樹  \n",
              "3                       ちょ、あの、オカン、これ水風呂やねんけど、なんの冗談??  \n",
              "4  「今日は皆さんにザリガニと消防車の違いを知ってもらいたいと思います」『どっちも同じだろ。両方...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-34b12ceb-5b26-4bbd-9621-70bb2386fac3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>odai_photo_file_name</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>rfdjcfsqq</td>\n",
              "      <td>nc1kez326b.jpg</td>\n",
              "      <td>僕のママ、キャラ弁のゆでたまごに８時間かかったんだ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>tsgqmfpef</td>\n",
              "      <td>49xt2fmjw0.jpg</td>\n",
              "      <td>かわいいが作れた！</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>owjcthkz2</td>\n",
              "      <td>9dtscjmyfh.jpg</td>\n",
              "      <td>来世の志茂田景樹</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>rvgaocjyy</td>\n",
              "      <td>osa3n56tiv.jpg</td>\n",
              "      <td>ちょ、あの、オカン、これ水風呂やねんけど、なんの冗談??</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>uxtwu5i69</td>\n",
              "      <td>yb1yqs4pvb.jpg</td>\n",
              "      <td>「今日は皆さんにザリガニと消防車の違いを知ってもらいたいと思います」『どっちも同じだろ。両方...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-34b12ceb-5b26-4bbd-9621-70bb2386fac3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-34b12ceb-5b26-4bbd-9621-70bb2386fac3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-34b12ceb-5b26-4bbd-9621-70bb2386fac3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Text Features\n",
        "\n",
        "続いてボケての文章について、BERTモデルを用いて特徴量化していきます。\n",
        "特徴量化については、以下のディスカッションを参考にさせていただきます。  \n",
        "[japanese-roberta-baseでテキストデータをembeddingする(小説家になろう ブクマ数予測 \\~”伸びる”タイトルとは？\\~ より)](https://www.nishika.com/competitions/21/topics/163)\n"
      ],
      "metadata": {
        "id": "cJAXTezHLBm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel"
      ],
      "metadata": {
        "id": "TcplAURxLwQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    replaced_text = text.lower()\n",
        "    replaced_text = re.sub(r'[【】]', ' ', replaced_text)       # 【】の除去\n",
        "    replaced_text = re.sub(r'[（）()]', ' ', replaced_text)     # （）の除去\n",
        "    replaced_text = re.sub(r'[［］\\[\\]]', ' ', replaced_text)   # ［］の除去\n",
        "    replaced_text = re.sub(r'[『』]', ' ', replaced_text)   # 『』の除去\n",
        "    replaced_text = re.sub(r'[@＠]\\w+', '', replaced_text)  # メンションの除去\n",
        "    replaced_text = re.sub(r'https?:\\/\\/.*?[\\r\\n ]', '', replaced_text)  # URLの除去\n",
        "    replaced_text = re.sub(r'　', ' ', replaced_text)  # 全角空白の除去\n",
        "    replaced_text = re.sub(r' ', '', replaced_text)  # 空白の除去\n",
        "    return replaced_text\n",
        "\n",
        "\n",
        "def clean_html_tags(html_text):\n",
        "    soup = BeautifulSoup(html_text, 'html.parser')\n",
        "    cleaned_text = soup.get_text()\n",
        "    cleaned_text = ''.join(cleaned_text.splitlines())\n",
        "    return cleaned_text\n",
        "\n",
        "\n",
        "def clean_html_and_js_tags(html_text):\n",
        "    soup = BeautifulSoup(html_text, 'html.parser')\n",
        "    [x.extract() for x in soup.findAll(['script', 'style'])]\n",
        "    cleaned_text = soup.get_text()\n",
        "    cleaned_text = ''.join(cleaned_text.splitlines())\n",
        "    return cleaned_text\n",
        "\n",
        "\n",
        "def clean_url(html_text):\n",
        "    cleaned_text = re.sub(r'http\\S+', '', html_text)\n",
        "    return cleaned_text\n",
        "\n",
        "\n",
        "def normalize(text):\n",
        "    normalized_text = normalize_unicode(text)\n",
        "    normalized_text = normalize_number(normalized_text)\n",
        "    normalized_text = lower_text(normalized_text)\n",
        "    return normalized_text\n",
        "\n",
        "\n",
        "def lower_text(text):\n",
        "    return text.lower()\n",
        "\n",
        "\n",
        "def normalize_unicode(text, form='NFKC'):\n",
        "    normalized_text = unicodedata.normalize(form, text)\n",
        "    return normalized_text\n",
        "\n",
        "\n",
        "def normalize_number(text):\n",
        "    replaced_text = re.sub(r'\\d+', '0', text)\n",
        "    return replaced_text\n",
        "\n",
        "\n",
        "def text_cleaning(text):\n",
        "    text = clean_text(text)\n",
        "    text = clean_html_tags(text)\n",
        "    text = clean_html_and_js_tags(text)\n",
        "    text = clean_url(text)\n",
        "    text = normalize(text)\n",
        "    text = lower_text(text)\n",
        "    text = normalize_unicode(text)\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "vJUPWqtXK_k_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from DeBERTa import deberta"
      ],
      "metadata": {
        "id": "KsJ31P4QSI0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DebertaV2Tokenizer"
      ],
      "metadata": {
        "id": "DIQ_ExzCMWiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BertSequenceVectorizer:\n",
        "    def __init__(self, model_name: str):\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        self.model_name = model_name\n",
        "        self.tokenizer = transformers.DebertaV2Tokenizer.from_pretrained(self.model_name)\n",
        "        self.tokenizer.do_lower_case = True \n",
        "        self.bert_model = AutoModel.from_pretrained(self.model_name)\n",
        "        self.bert_model = self.bert_model.to(self.device)\n",
        "        self.max_len = 256\n",
        "\n",
        "\n",
        "    def vectorize(self, sentence : str) -> np.array:\n",
        "        inp = self.tokenizer.encode(sentence)\n",
        "        len_inp = len(inp)\n",
        "\n",
        "        if len_inp >= self.max_len:\n",
        "            inputs = inp[:self.max_len]\n",
        "            masks = [1] * self.max_len\n",
        "        else:\n",
        "            inputs = inp + [0] * (self.max_len - len_inp)\n",
        "            masks = [1] * len_inp + [0] * (self.max_len - len_inp)\n",
        "\n",
        "        inputs_tensor = torch.tensor([inputs], dtype=torch.long).to(self.device)\n",
        "        masks_tensor = torch.tensor([masks], dtype=torch.long).to(self.device)\n",
        "\n",
        "        bert_out = self.bert_model(inputs_tensor, masks_tensor)\n",
        "        seq_out= bert_out['last_hidden_state']\n",
        "\n",
        "        if torch.cuda.is_available():    \n",
        "            return seq_out[0][0].cpu().detach().numpy()\n",
        "        else:\n",
        "            return seq_out[0][0].detach().numpy()"
      ],
      "metadata": {
        "id": "bK4VlYAFLZI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BSV = BertSequenceVectorizer(\"microsoft/deberta-v3-large\")\n",
        "\n",
        "## テキストの欠損値を補間します\n",
        "train_df[\"text\"] = train_df[\"text\"].fillna('NaN')\n",
        "test_df[\"text\"] = test_df[\"text\"].fillna('NaN')\n",
        "\n",
        "## BERT特徴量 \n",
        "features_text_train = np.stack(train_df[\"text\"].fillna(\"\").map(lambda x: BSV.vectorize(x).reshape(-1)).values)\n",
        "features_text_test = np.stack(test_df[\"text\"].fillna(\"\").map(lambda x: BSV.vectorize(x).reshape(-1)).values)"
      ],
      "metadata": {
        "id": "GIor4FKq1hbU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "116c849c-80a6-448a-9637-d7964e14545d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias']\n",
            "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## テキスト特徴量\n",
        "features_text_train_df = pd.DataFrame(features_text_train).add_prefix(\"debertav3-text\")\n",
        "features_text_test_df = pd.DataFrame(features_text_test).add_prefix(\"debertav3-text\")\n",
        "\n",
        "train_df = pd.concat([train_df, features_text_train_df], axis=1)\n",
        "test_df = pd.concat([test_df, features_text_test_df], axis=1)"
      ],
      "metadata": {
        "id": "U1I7xF2Y19MX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_df.shape)\n",
        "print(test_df.shape)"
      ],
      "metadata": {
        "id": "i-dWAwir54xN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "784ec282-9a7b-4797-94dd-7c20255305ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(24962, 1028)\n",
            "(6000, 1027)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.to_csv('/content/drive/MyDrive/nishika/embeded/embedding_train_debertav3.csv')"
      ],
      "metadata": {
        "id": "Hj68OivBX6sP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.to_csv('/content/drive/MyDrive/nishika/embeded/embedding_test_debertav3.csv')"
      ],
      "metadata": {
        "id": "LsxA0p2yYKPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df[\"is_laugh\"].value_counts()"
      ],
      "metadata": {
        "id": "fL4aJeMLYUWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Split"
      ],
      "metadata": {
        "id": "H4K63PLc6oGW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 学習データと評価データに分割します\n",
        "train_df, valid_df = train_test_split(train_df, test_size=0.2, random_state=42, stratify=train_df[\"is_laugh\"])\n",
        "\n",
        "train_y = train_df[\"is_laugh\"]\n",
        "train_x = train_df.drop([\"id\", \"odai_photo_file_name\", \"text\",\"is_laugh\"], axis=1)\n",
        "\n",
        "valid_y = valid_df[\"is_laugh\"]\n",
        "valid_x = valid_df.drop([\"id\", \"odai_photo_file_name\", \"text\",\"is_laugh\"], axis=1)\n",
        "\n",
        "test_x = test_df.drop([\"id\", \"odai_photo_file_name\", \"text\"], axis=1)"
      ],
      "metadata": {
        "id": "0pInRB316fFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_x.shape)\n",
        "print(valid_x.shape)"
      ],
      "metadata": {
        "id": "2DyOqFn57PAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "jtw_IUp87-HP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lgbm_params = {  \n",
        "    \"n_estimators\": 20000,\n",
        "    \"objective\": 'binary',\n",
        "    \"learning_rate\": 0.05,\n",
        "    \"num_leaves\": 32,\n",
        "    \"random_state\": 71,\n",
        "    \"n_jobs\": -1,\n",
        "    \"importance_type\": \"gain\",\n",
        "    'colsample_bytree': .8,\n",
        "    \"reg_lambda\": 5,\n",
        "    \"max_depth\":5,\n",
        "    }\n",
        "\n",
        "lgtrain = lgb.Dataset(train_x, train_y)\n",
        "lgvalid = lgb.Dataset(valid_x, valid_y)\n",
        "\n",
        "lgb_clf = lgb.train(\n",
        "    lgbm_params,\n",
        "    lgtrain,\n",
        "    num_boost_round=10000,\n",
        "    valid_sets=[lgtrain, lgvalid],\n",
        "    valid_names=['train','valid'],\n",
        "    early_stopping_rounds=50,\n",
        "    verbose_eval=50\n",
        ")"
      ],
      "metadata": {
        "id": "dZS_euCU79xd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 特徴量の重要度を可視化。\n",
        "lgb.plot_importance(lgb_clf, figsize=(12,8), max_num_features=50, importance_type='gain')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5m1V_v6T9vMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 評価指標はlog lossだが、accuracyも見てみる\n",
        "\n",
        "val_pred = lgb_clf.predict(valid_x, num_iteration=lgb_clf.best_iteration)\n",
        "val_pred_max = np.round(lgb_clf.predict(valid_x)).astype(int)  # クラスに分類\n",
        "accuracy = sum(valid_y == val_pred_max) / len(valid_y)\n",
        "print(accuracy)"
      ],
      "metadata": {
        "id": "qQaVWqkTAZ74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_conf_options = {\"normalize\": None,}\n",
        "_plot_options = {\n",
        "        \"cmap\": \"Blues\",\n",
        "        \"annot\": True\n",
        "    }\n",
        "\n",
        "conf = confusion_matrix(y_true=valid_y,\n",
        "                        y_pred=val_pred_max,\n",
        "                        **_conf_options)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "sns.heatmap(conf, ax=ax, **_plot_options)\n",
        "ax.set_ylabel(\"Label\")\n",
        "ax.set_xlabel(\"Predict\")"
      ],
      "metadata": {
        "id": "FPX2pXaZBMpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predict"
      ],
      "metadata": {
        "id": "sBad8xkWAXi1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_pred = lgb_clf.predict(test_x, num_iteration=lgb_clf.best_iteration)"
      ],
      "metadata": {
        "id": "gGvSdvMH-hq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission_df[\"is_laugh\"] = test_pred\n",
        "submission_df.head()"
      ],
      "metadata": {
        "id": "p8g6BTPjCg9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission_df.to_csv(('/content/drive/MyDrive/nishika/sub.csv'), index=False)"
      ],
      "metadata": {
        "id": "mEhTo6Leu02p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "実際に提出して、スコアを確認してみましょう。  精度向上に向けて様々なアイディアがあるかと思いますので、ぜひいろいろとトライしていただければと思います！\n",
        "\n",
        "- 異なる学習済みモデルでの特徴量化\n",
        "- 画像の状況とボケての文章との解離具合を測定する\n",
        "- 説明文口調とセリフ口調の分類をしてみる。\n",
        "- 画像に何が写っているかを検出し、特徴量に加えてみる（人が写っている。動物が写っている）\n"
      ],
      "metadata": {
        "id": "VSDaDtajVF3F"
      }
    }
  ]
}
