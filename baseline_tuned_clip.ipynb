{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exPnU6gJ9_8L"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9onj0RDDiWU4"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23tLb36S_nX-"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet transformers==4.18.0\n",
        "!pip install --quiet tokenizers==0.12.1\n",
        "!pip install --quiet sentencepiece\n",
        "!pip install --quiet japanize-matplotlib\n",
        "!pip install transformers fugashi ipadic >> /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgFkrWO3LVwp"
      },
      "outputs": [],
      "source": [
        "! pip install git+https://github.com/rinnakk/japanese-clip.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thcWprDW8bcF"
      },
      "outputs": [],
      "source": [
        "! conda install --yes -c pytorch pytorch=1.7.1 torchvision cudatoolkit=11.0\n",
        "! pip install ftfy regex tqdm\n",
        "! pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdWnvMGQO1Wa"
      },
      "outputs": [],
      "source": [
        "! conda install --yes -c pytorch pytorch=1.7.1 torchvision cudatoolkit=11.0\n",
        "! pip install ftfy regex tqdm\n",
        "! pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Myu_A_Kc_zyK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import transformers\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.model_selection import StratifiedKFold, KFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import re\n",
        "import random\n",
        "\n",
        "from time import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "from contextlib import contextmanager\n",
        "import lightgbm as lgb\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "import re\n",
        "import requests\n",
        "import unicodedata\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from bs4 import BeautifulSoup\n",
        "nltk.download(['wordnet', 'stopwords', 'punkt'])\n",
        "\n",
        "import japanese_clip as ja_clip\n",
        "from torchvision.io import read_image\n",
        "\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#上限表示数を拡張\n",
        "pd.set_option('display.max_columns', 200)\n",
        "pd.set_option('display.max_rows', 300)"
      ],
      "metadata": {
        "id": "BD85pPiXoRKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9WsZjx-NT-O"
      },
      "outputs": [],
      "source": [
        "def seed_everything(seed=1234):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "\n",
        "seed_everything(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajiBxfC2ABC4"
      },
      "outputs": [],
      "source": [
        "INPUT = \"/content/drive/MyDrive/nishika\" # 所望のディレクトリに変更してください。\n",
        "train_image_path =\"/content/drive/MyDrive/nishika/train/\"\n",
        "test_image_path =\"/content/drive/MyDrive/nishika/test/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nh06H3rFAPA5"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv(os.path.join(INPUT, \"train.csv\"))\n",
        "test_df = pd.read_csv(os.path.join(INPUT, \"test.csv\"))\n",
        "submission_df = pd.read_csv(os.path.join(INPUT, \"sample_submission.csv\"))\n",
        "\n",
        "train_df[\"img_path\"] = train_image_path + train_df[\"odai_photo_file_name\"]\n",
        "test_df[\"img_path\"] = test_image_path + test_df[\"odai_photo_file_name\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vyriHV_X8EcV"
      },
      "outputs": [],
      "source": [
        "import torchvision.models as models\n",
        "\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import unicode_literals\n",
        "import re\n",
        "import unicodedata\n",
        "\n",
        "def unicode_normalize(cls, s):\n",
        "    pt = re.compile('([{}]+)'.format(cls))\n",
        "\n",
        "    def norm(c):\n",
        "        return unicodedata.normalize('NFKC', c) if pt.match(c) else c\n",
        "\n",
        "    s = ''.join(norm(x) for x in re.split(pt, s))\n",
        "    s = re.sub('－', '-', s)\n",
        "    return s\n",
        "\n",
        "def remove_extra_spaces(s):\n",
        "    s = re.sub('[ 　]+', ' ', s)\n",
        "    blocks = ''.join(('\\u4E00-\\u9FFF',  # CJK UNIFIED IDEOGRAPHS\n",
        "                      '\\u3040-\\u309F',  # HIRAGANA\n",
        "                      '\\u30A0-\\u30FF',  # KATAKANA\n",
        "                      '\\u3000-\\u303F',  # CJK SYMBOLS AND PUNCTUATION\n",
        "                      '\\uFF00-\\uFFEF'   # HALFWIDTH AND FULLWIDTH FORMS\n",
        "                      ))\n",
        "    basic_latin = '\\u0000-\\u007F'\n",
        "\n",
        "    def remove_space_between(cls1, cls2, s):\n",
        "        p = re.compile('([{}]) ([{}])'.format(cls1, cls2))\n",
        "        while p.search(s):\n",
        "            s = p.sub(r'\\1\\2', s)\n",
        "        return s\n",
        "\n",
        "    s = remove_space_between(blocks, blocks, s)\n",
        "    s = remove_space_between(blocks, basic_latin, s)\n",
        "    s = remove_space_between(basic_latin, blocks, s)\n",
        "    return s\n",
        "\n",
        "def normalize_neologd(s):\n",
        "    s = s.strip()\n",
        "    s = unicode_normalize('０-９Ａ-Ｚａ-ｚ｡-ﾟ', s)\n",
        "\n",
        "    def maketrans(f, t):\n",
        "        return {ord(x): ord(y) for x, y in zip(f, t)}\n",
        "\n",
        "    s = re.sub('[˗֊‐‑‒–⁃⁻₋−]+', '-', s)  # normalize hyphens\n",
        "    s = re.sub('[﹣－ｰ—―─━ー]+', 'ー', s)  # normalize choonpus\n",
        "    s = re.sub('[~∼∾〜〰～]+', '〜', s)  # normalize tildes (modified by Isao Sonobe)\n",
        "    s = s.translate(\n",
        "        maketrans('!\"#$%&\\'()*+,-./:;<=>?@[¥]^_`{|}~｡､･｢｣',\n",
        "              '！”＃＄％＆’（）＊＋，－．／：；＜＝＞？＠［￥］＾＿｀｛｜｝〜。、・「」'))\n",
        "\n",
        "    s = remove_extra_spaces(s)\n",
        "    s = unicode_normalize('！”＃＄％＆’（）＊＋，－．／：；＜＞？＠［￥］＾＿｀｛｜｝〜', s)  # keep ＝,・,「,」\n",
        "    s = re.sub('[’]', '\\'', s)\n",
        "    s = re.sub('[”]', '\"', s)\n",
        "    s = s.lower()\n",
        "    return s\n",
        "\n",
        "def normalize_text(text):\n",
        "    return normalize_neologd(text)"
      ],
      "metadata": {
        "id": "IFyraA1X2DPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uemCcXr779IK"
      },
      "outputs": [],
      "source": [
        "#import clip\n",
        "\n",
        "model, preprocess = ja_clip.load(\"rinna/japanese-clip-vit-b-16\", cache_dir=\"/tmp/japanese_clip\", device=device)\n",
        "tokenizer = ja_clip.load_tokenizer()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WI4k4jz72NZ"
      },
      "outputs": [],
      "source": [
        "def _get_img_paths(img_dir):\n",
        "    img_dir = Path(img_dir)\n",
        "    img_extensions = [\".jpg\"]\n",
        "    img_paths = [str(p) for p in img_dir.iterdir() if p.suffix in img_extensions]\n",
        "    img_paths.sort()\n",
        "\n",
        "    return img_paths\n",
        "\n",
        "\n",
        "class ImageFolder(Dataset):\n",
        "    def __init__(self, img_dir):\n",
        "        # 画像ファイルのパス一覧を取得する。\n",
        "        self.img_paths = _get_img_paths(img_dir)\n",
        "        #self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        path = self.img_paths[index]\n",
        "        img = preprocess(Image.open(path).convert(\"RGB\"))\n",
        "        #inputs = self.transform(img)\n",
        "\n",
        "        return {\"image\": img, \"path\": path}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ObnpqCS9Bvui"
      },
      "outputs": [],
      "source": [
        "# Dataset を作成する。\n",
        "dstrain = ImageFolder(\"/content/drive/MyDrive/nishika/train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJkZmjM1ANhJ"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import itertools\n",
        "\n",
        "\n",
        "def get_images_features(dataset):\n",
        "    image_features = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(DataLoader(dataset, batch_size=16)):\n",
        "            inputs = batch[\"image\"].to(device)\n",
        "            outputs = model.get_image_features(inputs)\n",
        "\n",
        "            image_features.append(outputs)\n",
        "\n",
        "    return torch.cat(image_features).cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uf2ZalqQFcau"
      },
      "outputs": [],
      "source": [
        "train_image_features = get_images_features(dstrain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T3x9JsVsI3ht"
      },
      "outputs": [],
      "source": [
        "image_feature = pd.DataFrame.from_dict(train_image_features, orient='columns').add_prefix(\"clip_image_\").reset_index()\n",
        "image_feature.rename(columns={\"columns\":\"odai_photo_file_name\"}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJ9ot5R6YWge"
      },
      "outputs": [],
      "source": [
        "image_feature = image_feature.drop([\"index\"], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNZpZ19iJEun"
      },
      "outputs": [],
      "source": [
        "# trainのデータに結合します。\n",
        "train_df = pd.concat([train_df, image_feature],axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLa8BVy5y4mh"
      },
      "outputs": [],
      "source": [
        "train_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zDcOsVqB984D"
      },
      "outputs": [],
      "source": [
        "# Dataset を作成する。\n",
        "dstest = ImageFolder(\"/content/drive/MyDrive/nishika/test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JcwVrtW_-D0t"
      },
      "outputs": [],
      "source": [
        "test_image_features = get_images_features(dstest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNVN31Wr-J9F"
      },
      "outputs": [],
      "source": [
        "image_feature = pd.DataFrame.from_dict(test_image_features, orient='columns').add_prefix(\"clip_image_\").reset_index()\n",
        "image_feature.rename(columns={\"columns\":\"odai_photo_file_name\"}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I99tWAshZNYm"
      },
      "outputs": [],
      "source": [
        "print(image_feature)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KditMK0ZDTW"
      },
      "outputs": [],
      "source": [
        "image_feature = image_feature.drop([\"index\"], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXHqKW1-ZQyW"
      },
      "outputs": [],
      "source": [
        "print(image_feature)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1riWq283-Plj"
      },
      "outputs": [],
      "source": [
        "# testのデータに結合します。\n",
        "test_df = pd.concat([test_df, image_feature],axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xtltEAbzoko",
        "outputId": "da0a5b9c-d916-4fd9-ce76-7ec651f5a477"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6000, 516)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "test_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VwWPcX3rr8PW"
      },
      "outputs": [],
      "source": [
        "## テキストの欠損値を補間します\n",
        "train_df[\"text\"] = train_df[\"text\"].fillna('NaN')\n",
        "test_df[\"text\"] = test_df[\"text\"].fillna('NaN')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32m2iyR0kqM2"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def get_texts_features(dataset):\n",
        "    text_labels = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for labels in tqdm(DataLoader(dataset, batch_size=16)):\n",
        "            inputs = ja_clip.tokenize(texts = [normalize_text(label) for label in labels], device = device)\n",
        "            outputs = model.get_text_features(**inputs)\n",
        "            text_labels.append(outputs)\n",
        "\n",
        "    return torch.cat(text_labels).cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2tYXFzYlCOq"
      },
      "outputs": [],
      "source": [
        "train_labels = get_texts_features(train_df[\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXRQoEBXz0Zj"
      },
      "outputs": [],
      "source": [
        "test_labels = get_texts_features(test_df[\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7fXmhjUtz9u5"
      },
      "outputs": [],
      "source": [
        "print(train_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1I7xF2Y19MX"
      },
      "outputs": [],
      "source": [
        "## テキスト特徴量\n",
        "features_text_train_df = pd.DataFrame(train_labels).add_prefix(\"clip-text\")\n",
        "features_text_test_df = pd.DataFrame(test_labels).add_prefix(\"clip-text\")\n",
        "\n",
        "train_df = pd.concat([train_df, features_text_train_df], axis=1)\n",
        "test_df = pd.concat([test_df, features_text_test_df], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-dWAwir54xN"
      },
      "outputs": [],
      "source": [
        "print(train_df.shape)\n",
        "print(test_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hj68OivBX6sP"
      },
      "outputs": [],
      "source": [
        "train_df.to_csv('/content/drive/MyDrive/nishika/embeded/embedding_train_rinna_tune_clip.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LsxA0p2yYKPT"
      },
      "outputs": [],
      "source": [
        "test_df.to_csv('/content/drive/MyDrive/nishika/embeded/embedding_test_rinna_tune_clip.csv')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}