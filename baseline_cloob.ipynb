{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0ayzrjm-DgE"
      },
      "source": [
        "# ボケ判定AIを作ろう！-チュートリアル1\n",
        "このnotebookは、Nishikaコンペティション [ボケ判定AIを作ろう！](https://www.nishika.com/competitions/) のチュートリアルです。\n",
        "\n",
        "「ボケて」データを用いて、画像データと文章からそのボケてが面白いか面白くないかを予測することをテーマとしています。\n",
        "\n",
        "このNotebookでは、画像とテキストそれぞれの特徴量生成を以下のような方法で行っていきます。\n",
        "\n",
        "- CNNモデルを用いた画像データの特徴量化\n",
        "- BERTモデルを用いたテキストデータの特徴量化\n",
        "\n",
        "特徴量の作成では、テキストと画像それぞれ別々で作成していますので、画像データとテキストデータを組み合わせた特徴量を入れることで精度向上が見込めるかも知れませんので、いろいろと試していただければと思います。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjvjywgsCuKU"
      },
      "source": [
        "| 要素 | 説明 |\n",
        "| ---- | ---- |\n",
        "|id | ID|\n",
        "|odai_photo_file_name | ボケてのお題画像|\n",
        "|text | ボケての文章|\n",
        "|is_laugh | 面白さ（面白い：１、面白くない：０）|\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YJVABJ8CqoX"
      },
      "source": [
        "ディレクトリ構成は以下のように設定します\n",
        "\n",
        "```\n",
        "├── train.zip\n",
        "│ ├── xxx.jpg\n",
        "│ └── yyy.jpg\n",
        "├── test.zip\n",
        "│ ├── xxx.jpg\n",
        "│ └── yyy.jpg\n",
        "├── train.csv\n",
        "├── test.csv\n",
        "├── sample_submission.csv\n",
        "└── submission.csv(今回のbaselineで生成されるsubmissionファイル)\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "### setting\n",
        "ページ上部の「ランタイム」>「ランタイムのタイプを変更」から「GPU」「ハイメモリ」を選択"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exPnU6gJ9_8L",
        "outputId": "9bf33d3c-a275-49b6-cbbd-4a0ddf14fed6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9onj0RDDiWU4",
        "outputId": "f6e5c0df-8727-47c3-a446-fa7c74aaea27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Sep 21 05:07:57 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   54C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGai1EPR_tEA"
      },
      "source": [
        "# Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "23tLb36S_nX-"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet transformers==4.18.0\n",
        "!pip install --quiet tokenizers==0.12.1\n",
        "!pip install --quiet sentencepiece\n",
        "!pip install --quiet japanize-matplotlib\n",
        "!pip install transformers fugashi ipadic >> /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QgFkrWO3LVwp",
        "outputId": "3f267d8f-c2e7-4bce-98c7-ea677058ba9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/rinnakk/japanese-clip.git\n",
            "  Cloning https://github.com/rinnakk/japanese-clip.git to /tmp/pip-req-build-sw7r1w0c\n",
            "  Running command git clone -q https://github.com/rinnakk/japanese-clip.git /tmp/pip-req-build-sw7r1w0c\n",
            "Requirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from japanese-clip==0.2.0) (1.12.1+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from japanese-clip==0.2.0) (0.13.1+cu113)\n",
            "Requirement already satisfied: transformers>=4.15.0 in /usr/local/lib/python3.7/dist-packages (from japanese-clip==0.2.0) (4.18.0)\n",
            "Requirement already satisfied: sentencepiece<=0.1.94,>=0.1.91 in /usr/local/lib/python3.7/dist-packages (from japanese-clip==0.2.0) (0.1.94)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from japanese-clip==0.2.0) (1.3.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from japanese-clip==0.2.0) (1.7.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.9.0->japanese-clip==0.2.0) (4.1.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.15.0->japanese-clip==0.2.0) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.15.0->japanese-clip==0.2.0) (4.64.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.15.0->japanese-clip==0.2.0) (0.9.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.15.0->japanese-clip==0.2.0) (21.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.15.0->japanese-clip==0.2.0) (0.12.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.15.0->japanese-clip==0.2.0) (2022.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers>=4.15.0->japanese-clip==0.2.0) (2.23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.15.0->japanese-clip==0.2.0) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=4.15.0->japanese-clip==0.2.0) (3.8.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers>=4.15.0->japanese-clip==0.2.0) (4.12.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers>=4.15.0->japanese-clip==0.2.0) (0.0.53)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers>=4.15.0->japanese-clip==0.2.0) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers>=4.15.0->japanese-clip==0.2.0) (3.8.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->japanese-clip==0.2.0) (2022.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->japanese-clip==0.2.0) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->japanese-clip==0.2.0) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=4.15.0->japanese-clip==0.2.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=4.15.0->japanese-clip==0.2.0) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=4.15.0->japanese-clip==0.2.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=4.15.0->japanese-clip==0.2.0) (1.24.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=4.15.0->japanese-clip==0.2.0) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=4.15.0->japanese-clip==0.2.0) (7.1.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->japanese-clip==0.2.0) (7.1.2)\n"
          ]
        }
      ],
      "source": [
        "! pip install git+https://github.com/rinnakk/japanese-clip.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thcWprDW8bcF",
        "outputId": "3d7aae02-9888-4d91-e3d3-84fb483b620b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: conda: command not found\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.64.1)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy) (0.2.5)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-abiu76v3\n",
            "  Running command git clone -q https://github.com/openai/CLIP.git /tmp/pip-req-build-abiu76v3\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (4.64.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (1.12.1+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (0.13.1+cu113)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy->clip==1.0) (0.2.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->clip==1.0) (4.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (1.21.6)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (7.1.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->clip==1.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->clip==1.0) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->clip==1.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->clip==1.0) (2.10)\n"
          ]
        }
      ],
      "source": [
        "! conda install --yes -c pytorch pytorch=1.7.1 torchvision cudatoolkit=11.0\n",
        "! pip install ftfy regex tqdm\n",
        "! pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UdWnvMGQO1Wa",
        "outputId": "d4f8bdf1-3620-43e5-cc45-e7b8523c188c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: conda: command not found\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.64.1)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy) (0.2.5)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-r2ctdue1\n",
            "  Running command git clone -q https://github.com/openai/CLIP.git /tmp/pip-req-build-r2ctdue1\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (4.64.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (1.12.1+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (0.13.1+cu113)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy->clip==1.0) (0.2.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->clip==1.0) (4.1.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (1.21.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->clip==1.0) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->clip==1.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->clip==1.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->clip==1.0) (1.24.3)\n"
          ]
        }
      ],
      "source": [
        "! conda install --yes -c pytorch pytorch=1.7.1 torchvision cudatoolkit=11.0\n",
        "! pip install ftfy regex tqdm\n",
        "! pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Myu_A_Kc_zyK",
        "outputId": "8f71a5bc-e062-42c0-981b-38aedc251550"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import transformers\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.model_selection import StratifiedKFold, KFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import re\n",
        "import random\n",
        "\n",
        "from time import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "from contextlib import contextmanager\n",
        "import lightgbm as lgb\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "import re\n",
        "import requests\n",
        "import unicodedata\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from bs4 import BeautifulSoup\n",
        "nltk.download(['wordnet', 'stopwords', 'punkt'])\n",
        "\n",
        "import japanese_clip as ja_clip\n",
        "from torchvision.io import read_image\n",
        "\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCYVrA1lAB8L"
      },
      "source": [
        "# Setting"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#上限表示数を拡張\n",
        "pd.set_option('display.max_columns', 200)\n",
        "pd.set_option('display.max_rows', 300)"
      ],
      "metadata": {
        "id": "BD85pPiXoRKL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "z9WsZjx-NT-O"
      },
      "outputs": [],
      "source": [
        "def seed_everything(seed=1234):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "\n",
        "seed_everything(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ajiBxfC2ABC4"
      },
      "outputs": [],
      "source": [
        "INPUT = \"/content/drive/MyDrive/nishika\" # 所望のディレクトリに変更してください。\n",
        "train_image_path =\"/content/drive/MyDrive/nishika/train/\"\n",
        "test_image_path =\"/content/drive/MyDrive/nishika/test/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Nh06H3rFAPA5"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv(os.path.join(INPUT, \"train.csv\"))\n",
        "test_df = pd.read_csv(os.path.join(INPUT, \"test.csv\"))\n",
        "submission_df = pd.read_csv(os.path.join(INPUT, \"sample_submission.csv\"))\n",
        "\n",
        "train_df[\"img_path\"] = train_image_path + train_df[\"odai_photo_file_name\"]\n",
        "test_df[\"img_path\"] = test_image_path + test_df[\"odai_photo_file_name\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "vyriHV_X8EcV"
      },
      "outputs": [],
      "source": [
        "import torchvision.models as models\n",
        "\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "uemCcXr779IK"
      },
      "outputs": [],
      "source": [
        "model, preprocess = ja_clip.load(\"rinna/japanese-cloob-vit-b-16\", device=device)\n",
        "tokenizer = ja_clip.load_tokenizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "2WI4k4jz72NZ"
      },
      "outputs": [],
      "source": [
        "def _get_img_paths(img_dir):\n",
        "    img_dir = Path(img_dir)\n",
        "    img_extensions = [\".jpg\"]\n",
        "    img_paths = [str(p) for p in img_dir.iterdir() if p.suffix in img_extensions]\n",
        "    img_paths.sort()\n",
        "\n",
        "    return img_paths\n",
        "\n",
        "\n",
        "class ImageFolder(Dataset):\n",
        "    def __init__(self, img_dir):\n",
        "        # 画像ファイルのパス一覧を取得する。\n",
        "        self.img_paths = _get_img_paths(img_dir)\n",
        "        #self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        path = self.img_paths[index]\n",
        "        img = preprocess(Image.open(path).convert(\"RGB\"))\n",
        "        #inputs = self.transform(img)\n",
        "\n",
        "        return {\"image\": img, \"path\": path}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ObnpqCS9Bvui"
      },
      "outputs": [],
      "source": [
        "# Dataset を作成する。\n",
        "dstrain = ImageFolder(\"/content/drive/MyDrive/nishika/train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "RJkZmjM1ANhJ"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import itertools\n",
        "\n",
        "\n",
        "def get_images_features(dataset):\n",
        "    image_features = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(DataLoader(dataset, batch_size=8)):\n",
        "            inputs = batch[\"image\"].to(device)\n",
        "            outputs = model.get_image_features(inputs)\n",
        "\n",
        "            image_features.append(outputs)\n",
        "\n",
        "    return torch.cat(image_features).cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uf2ZalqQFcau",
        "outputId": "c31df80e-68d2-40de-ad65-69394bf8b679"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 96%|█████████▋| 3005/3121 [4:36:41<10:57,  5.67s/it]"
          ]
        }
      ],
      "source": [
        "train_image_features = get_images_features(dstrain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T3x9JsVsI3ht"
      },
      "outputs": [],
      "source": [
        "image_feature = pd.DataFrame.from_dict(train_image_features, orient='columns').add_prefix(\"cloob_image_\").reset_index()\n",
        "image_feature.rename(columns={\"columns\":\"odai_photo_file_name\"}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJ9ot5R6YWge"
      },
      "outputs": [],
      "source": [
        "image_feature = image_feature.drop([\"index\"], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNZpZ19iJEun"
      },
      "outputs": [],
      "source": [
        "# trainのデータに結合します。\n",
        "train_df = pd.concat([train_df, image_feature],axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLa8BVy5y4mh"
      },
      "outputs": [],
      "source": [
        "train_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zDcOsVqB984D"
      },
      "outputs": [],
      "source": [
        "# Dataset を作成する。\n",
        "dstest = ImageFolder(\"/content/drive/MyDrive/nishika/test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JcwVrtW_-D0t"
      },
      "outputs": [],
      "source": [
        "test_image_features = get_images_features(dstest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNVN31Wr-J9F"
      },
      "outputs": [],
      "source": [
        "image_feature = pd.DataFrame.from_dict(test_image_features, orient='columns').add_prefix(\"cloob_image_\").reset_index()\n",
        "image_feature.rename(columns={\"columns\":\"odai_photo_file_name\"}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I99tWAshZNYm"
      },
      "outputs": [],
      "source": [
        "print(image_feature)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KditMK0ZDTW"
      },
      "outputs": [],
      "source": [
        "image_feature = image_feature.drop([\"index\"], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXHqKW1-ZQyW"
      },
      "outputs": [],
      "source": [
        "print(image_feature)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1riWq283-Plj"
      },
      "outputs": [],
      "source": [
        "# testのデータに結合します。\n",
        "test_df = pd.concat([test_df, image_feature],axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xtltEAbzoko"
      },
      "outputs": [],
      "source": [
        "test_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.to_csv('/content/drive/MyDrive/nishika/embeded/embedding_train_image_rinna_cloob.csv')\n",
        "test_df.to_csv('/content/drive/MyDrive/nishika/embeded/embedding_test_image_rinna_cloob.csv')"
      ],
      "metadata": {
        "id": "fh8Q-8M6awdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VwWPcX3rr8PW"
      },
      "outputs": [],
      "source": [
        "## テキストの欠損値を補間します\n",
        "train_df[\"text\"] = train_df[\"text\"].fillna('NaN')\n",
        "test_df[\"text\"] = test_df[\"text\"].fillna('NaN')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32m2iyR0kqM2"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def get_texts_features(dataset):\n",
        "    text_labels = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for labels in tqdm(DataLoader(dataset, batch_size=8)):\n",
        "            inputs = ja_clip.tokenize(texts = labels, device = device)\n",
        "            outputs = model.get_text_features(**inputs)\n",
        "            text_labels.append(outputs)\n",
        "\n",
        "    return torch.cat(text_labels).cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2tYXFzYlCOq"
      },
      "outputs": [],
      "source": [
        "train_labels = get_texts_features(train_df[\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXRQoEBXz0Zj"
      },
      "outputs": [],
      "source": [
        "test_labels = get_texts_features(test_df[\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7fXmhjUtz9u5"
      },
      "outputs": [],
      "source": [
        "print(train_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1I7xF2Y19MX"
      },
      "outputs": [],
      "source": [
        "## テキスト特徴量\n",
        "features_text_train_df = pd.DataFrame(train_labels).add_prefix(\"cloob-text\")\n",
        "features_text_test_df = pd.DataFrame(test_labels).add_prefix(\"cloob-text\")\n",
        "\n",
        "train_df = pd.concat([train_df, features_text_train_df], axis=1)\n",
        "test_df = pd.concat([test_df, features_text_test_df], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-dWAwir54xN"
      },
      "outputs": [],
      "source": [
        "print(train_df.shape)\n",
        "print(test_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hj68OivBX6sP"
      },
      "outputs": [],
      "source": [
        "train_df.to_csv('/content/drive/MyDrive/nishika/embeded/embedding_train_rinna_cloob.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LsxA0p2yYKPT"
      },
      "outputs": [],
      "source": [
        "test_df.to_csv('/content/drive/MyDrive/nishika/embeded/embedding_test_rinna_cloob.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fL4aJeMLYUWT"
      },
      "outputs": [],
      "source": [
        "train_df[\"is_laugh\"].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4K63PLc6oGW"
      },
      "source": [
        "# Data Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pInRB316fFo"
      },
      "outputs": [],
      "source": [
        "# 学習データと評価データに分割します\n",
        "train_df, valid_df = train_test_split(train_df, test_size=0.2, random_state=42, stratify=train_df[\"is_laugh\"])\n",
        "\n",
        "train_y = train_df[\"is_laugh\"]\n",
        "train_x = train_df.drop([\"id\", \"odai_photo_file_name\", \"text\",\"is_laugh\"], axis=1)\n",
        "\n",
        "valid_y = valid_df[\"is_laugh\"]\n",
        "valid_x = valid_df.drop([\"id\", \"odai_photo_file_name\", \"text\",\"is_laugh\"], axis=1)\n",
        "\n",
        "test_x = test_df.drop([\"id\", \"odai_photo_file_name\", \"text\"], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DyOqFn57PAA"
      },
      "outputs": [],
      "source": [
        "print(train_x.shape)\n",
        "print(valid_x.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtw_IUp87-HP"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_x = train_x.drop(\"img_path\", axis=1)\n",
        "valid_x = valid_x.drop(\"img_path\", axis=1)"
      ],
      "metadata": {
        "id": "p-rAzkqFyXvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_x = test_x.drop(\"img_path\", axis=1)"
      ],
      "metadata": {
        "id": "PhwSLHloyNaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZS_euCU79xd"
      },
      "outputs": [],
      "source": [
        "lgbm_params = {  \n",
        "    \"n_estimators\": 20000,\n",
        "    \"objective\": 'binary',\n",
        "    \"learning_rate\": 0.05,\n",
        "    \"num_leaves\": 32,\n",
        "    \"random_state\": 71,\n",
        "    \"n_jobs\": -1,\n",
        "    \"importance_type\": \"gain\",\n",
        "    'colsample_bytree': .8,\n",
        "    \"reg_lambda\": 5,\n",
        "    \"max_depth\":5,\n",
        "    }\n",
        "\n",
        "lgtrain = lgb.Dataset(train_x, train_y)\n",
        "lgvalid = lgb.Dataset(valid_x, valid_y)\n",
        "\n",
        "lgb_clf = lgb.train(\n",
        "    lgbm_params,\n",
        "    lgtrain,\n",
        "    num_boost_round=10000,\n",
        "    valid_sets=[lgtrain, lgvalid],\n",
        "    valid_names=['train','valid'],\n",
        "    early_stopping_rounds=100,\n",
        "    verbose_eval=100\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5m1V_v6T9vMz"
      },
      "outputs": [],
      "source": [
        "# 特徴量の重要度を可視化。\n",
        "lgb.plot_importance(lgb_clf, figsize=(12,8), max_num_features=50, importance_type='gain')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQaVWqkTAZ74"
      },
      "outputs": [],
      "source": [
        "# 評価指標はlog lossだが、accuracyも見てみる\n",
        "\n",
        "val_pred = lgb_clf.predict(valid_x, num_iteration=lgb_clf.best_iteration)\n",
        "val_pred_max = np.round(lgb_clf.predict(valid_x)).astype(int)  # クラスに分類\n",
        "accuracy = sum(valid_y == val_pred_max) / len(valid_y)\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPX2pXaZBMpu"
      },
      "outputs": [],
      "source": [
        "_conf_options = {\"normalize\": None,}\n",
        "_plot_options = {\n",
        "        \"cmap\": \"Blues\",\n",
        "        \"annot\": True\n",
        "    }\n",
        "\n",
        "conf = confusion_matrix(y_true=valid_y,\n",
        "                        y_pred=val_pred_max,\n",
        "                        **_conf_options)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "sns.heatmap(conf, ax=ax, **_plot_options)\n",
        "ax.set_ylabel(\"Label\")\n",
        "ax.set_xlabel(\"Predict\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBad8xkWAXi1"
      },
      "source": [
        "# Predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGvSdvMH-hq5"
      },
      "outputs": [],
      "source": [
        "test_pred = lgb_clf.predict(test_x, num_iteration=lgb_clf.best_iteration)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p8g6BTPjCg9R"
      },
      "outputs": [],
      "source": [
        "submission_df[\"is_laugh\"] = test_pred\n",
        "submission_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mEhTo6Leu02p"
      },
      "outputs": [],
      "source": [
        "submission_df.to_csv(('/content/drive/MyDrive/nishika/sub.csv'), index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSDaDtajVF3F"
      },
      "source": [
        "実際に提出して、スコアを確認してみましょう。  精度向上に向けて様々なアイディアがあるかと思いますので、ぜひいろいろとトライしていただければと思います！\n",
        "\n",
        "- 異なる学習済みモデルでの特徴量化\n",
        "- 画像の状況とボケての文章との解離具合を測定する\n",
        "- 説明文口調とセリフ口調の分類をしてみる。\n",
        "- 画像に何が写っているかを検出し、特徴量に加えてみる（人が写っている。動物が写っている）\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}