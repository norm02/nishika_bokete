{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ボケ判定AIを作ろう！-チュートリアル1\n",
        "このnotebookは、Nishikaコンペティション [ボケ判定AIを作ろう！](https://www.nishika.com/competitions/) のチュートリアルです。\n",
        "\n",
        "「ボケて」データを用いて、画像データと文章からそのボケてが面白いか面白くないかを予測することをテーマとしています。\n",
        "\n",
        "このNotebookでは、画像とテキストそれぞれの特徴量生成を以下のような方法で行っていきます。\n",
        "\n",
        "- CNNモデルを用いた画像データの特徴量化\n",
        "- BERTモデルを用いたテキストデータの特徴量化\n",
        "\n",
        "特徴量の作成では、テキストと画像それぞれ別々で作成していますので、画像データとテキストデータを組み合わせた特徴量を入れることで精度向上が見込めるかも知れませんので、いろいろと試していただければと思います。"
      ],
      "metadata": {
        "id": "m0ayzrjm-DgE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| 要素 | 説明 |\n",
        "| ---- | ---- |\n",
        "|id | ID|\n",
        "|odai_photo_file_name | ボケてのお題画像|\n",
        "|text | ボケての文章|\n",
        "|is_laugh | 面白さ（面白い：１、面白くない：０）|\n"
      ],
      "metadata": {
        "id": "EjvjywgsCuKU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ディレクトリ構成は以下のように設定します\n",
        "\n",
        "```\n",
        "├── train.zip\n",
        "│ ├── xxx.jpg\n",
        "│ └── yyy.jpg\n",
        "├── test.zip\n",
        "│ ├── xxx.jpg\n",
        "│ └── yyy.jpg\n",
        "├── train.csv\n",
        "├── test.csv\n",
        "├── sample_submission.csv\n",
        "└── submission.csv(今回のbaselineで生成されるsubmissionファイル)\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "### setting\n",
        "ページ上部の「ランタイム」>「ランタイムのタイプを変更」から「GPU」「ハイメモリ」を選択"
      ],
      "metadata": {
        "id": "9YJVABJ8CqoX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exPnU6gJ9_8L",
        "outputId": "b231db44-0e82-4eb1-c4a1-3f8ee1e7a6a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9onj0RDDiWU4",
        "outputId": "fdb726a5-4fd0-45a5-bec6-6e9c44c420e0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Library"
      ],
      "metadata": {
        "id": "JGai1EPR_tEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet transformers==4.18.0\n",
        "!pip install --quiet tokenizers==0.12.1\n",
        "!pip install --quiet sentencepiece\n",
        "!pip install --quiet japanize-matplotlib\n",
        "! pip install transformers[ja] > /dev/null 2>&1"
      ],
      "metadata": {
        "id": "23tLb36S_nX-"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "import japanize_matplotlib\n",
        "\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import BertTokenizer, BertJapaneseTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.model_selection import StratifiedKFold, KFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import re\n",
        "import random\n",
        "\n",
        "from time import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "from contextlib import contextmanager\n",
        "import lightgbm as lgb\n",
        "\n",
        "import re\n",
        "import requests\n",
        "import unicodedata\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from bs4 import BeautifulSoup\n",
        "nltk.download(['wordnet', 'stopwords', 'punkt'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Myu_A_Kc_zyK",
        "outputId": "54250ed6-292c-4f31-bc38-4388b905a870"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting"
      ],
      "metadata": {
        "id": "dCYVrA1lAB8L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_everything(seed=1234):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "\n",
        "seed_everything(42)"
      ],
      "metadata": {
        "id": "z9WsZjx-NT-O"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT = \"/content/drive/MyDrive/nishika\" # 所望のディレクトリに変更してください。\n",
        "train_image_path = \"/content/drive/MyDrive/nishika/train/\"\n",
        "test_image_path = \"/content/drive/MyDrive/nishika/test/\""
      ],
      "metadata": {
        "id": "ajiBxfC2ABC4"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read Data\n",
        "学習データと推論データについて、目的変数の分布などを確認していきます。"
      ],
      "metadata": {
        "id": "qbdARhsQASHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(os.path.join(INPUT, \"train.csv\"))\n",
        "test_df = pd.read_csv(os.path.join(INPUT, \"test.csv\"))\n",
        "submission_df = pd.read_csv(os.path.join(INPUT, \"sample_submission.csv\"))"
      ],
      "metadata": {
        "id": "Nh06H3rFAPA5"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    replaced_text = text.lower()\n",
        "    replaced_text = re.sub(r'[【】]', ' ', replaced_text)       # 【】の除去\n",
        "    replaced_text = re.sub(r'[（）()]', ' ', replaced_text)     # （）の除去\n",
        "    replaced_text = re.sub(r'[［］\\[\\]]', ' ', replaced_text)   # ［］の除去\n",
        "    replaced_text = re.sub(r'[『』]', ' ', replaced_text)   # 『』の除去\n",
        "    replaced_text = re.sub(r'[@＠]\\w+', '', replaced_text)  # メンションの除去\n",
        "    replaced_text = re.sub(r'https?:\\/\\/.*?[\\r\\n ]', '', replaced_text)  # URLの除去\n",
        "    replaced_text = re.sub(r'　', ' ', replaced_text)  # 全角空白の除去\n",
        "    replaced_text = re.sub(r' ', '', replaced_text)  # 空白の除去\n",
        "    return replaced_text\n",
        "\n",
        "\n",
        "def clean_html_tags(html_text):\n",
        "    soup = BeautifulSoup(html_text, 'html.parser')\n",
        "    cleaned_text = soup.get_text()\n",
        "    cleaned_text = ''.join(cleaned_text.splitlines())\n",
        "    return cleaned_text\n",
        "\n",
        "\n",
        "def clean_html_and_js_tags(html_text):\n",
        "    soup = BeautifulSoup(html_text, 'html.parser')\n",
        "    [x.extract() for x in soup.findAll(['script', 'style'])]\n",
        "    cleaned_text = soup.get_text()\n",
        "    cleaned_text = ''.join(cleaned_text.splitlines())\n",
        "    return cleaned_text\n",
        "\n",
        "\n",
        "def clean_url(html_text):\n",
        "    cleaned_text = re.sub(r'http\\S+', '', html_text)\n",
        "    return cleaned_text\n",
        "\n",
        "\n",
        "def normalize(text):\n",
        "    normalized_text = normalize_unicode(text)\n",
        "    normalized_text = normalize_number(normalized_text)\n",
        "    normalized_text = lower_text(normalized_text)\n",
        "    return normalized_text\n",
        "\n",
        "\n",
        "def lower_text(text):\n",
        "    return text.lower()\n",
        "\n",
        "\n",
        "def normalize_unicode(text, form='NFKC'):\n",
        "    normalized_text = unicodedata.normalize(form, text)\n",
        "    return normalized_text\n",
        "\n",
        "\n",
        "def normalize_number(text):\n",
        "    replaced_text = re.sub(r'\\d+', '0', text)\n",
        "    return replaced_text\n",
        "\n",
        "\n",
        "def text_cleaning(text):\n",
        "    text = clean_text(text)\n",
        "    text = clean_html_tags(text)\n",
        "    text = clean_html_and_js_tags(text)\n",
        "    text = clean_url(text)\n",
        "    text = normalize(text)\n",
        "    text = lower_text(text)\n",
        "    text = normalize_unicode(text)\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "vJUPWqtXK_k_"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "ZN_qWMdoq9fC"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "sedPLBmytZl1",
        "outputId": "0e0f2637-00bc-4eb6-d6d8-0ca5649aa360"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained('daigo/bert-base-japanese-sentiment') \n",
        "tokenizer = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')"
      ],
      "metadata": {
        "id": "zml82K9koqkq"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_analyzer = pipeline(\"sentiment-analysis\", model=model.to(device), tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "1erxq6Amw14M"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(sentence):\n",
        "    res = sentiment_analyzer(sentence)[0]\n",
        "    score = res[\"score\"]\n",
        "    if res[\"label\"] != \"ポジティブ\":\n",
        "        score *= -1\n",
        "    return score    "
      ],
      "metadata": {
        "id": "W8riWt03xeFt"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df[\"pred\"] = train_df[\"text\"].apply(predict)\n",
        "test_df[\"pred\"] = test_df[\"text\"].apply(predict)"
      ],
      "metadata": {
        "id": "ehZc6LmkxDIP"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.to_csv(\"/content/drive/MyDrive/nishika/output/train_sentiment.csv\")"
      ],
      "metadata": {
        "id": "qe_9eIRW9POC"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.to_csv(\"/content/drive/MyDrive/nishika/output/test_sentiment.csv\")"
      ],
      "metadata": {
        "id": "BNVtPSmC9Zhp"
      },
      "execution_count": 27,
      "outputs": []
    }
  ]
}