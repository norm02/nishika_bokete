{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xxBFzShGILs"
      },
      "source": [
        "# ボケ判定AIを作ろう！-チュートリアル2\n",
        "このnotebookは、Nishikaコンペティション [ボケ判定AIを作ろう！](https://www.nishika.com/competitions/) のチュートリアルです。\n",
        "\n",
        "「ボケて」データを用いて、画像データと文章からそのボケてが面白いか面白くないかを予測することをテーマとしています。\n",
        "\n",
        "チュートリアル１では画像データとテキストデータを特徴量としてlightGBMで予測する方法を紹介しました。\n",
        "\n",
        "このNotebookでは、画像とテキストの両方を用いたマルチモーダル分類モデルとして公開されているMMBT（MultiModal BiTransformers）を用いて予測するモデルを紹介します。\n"
      ],
      "id": "7xxBFzShGILs"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0qqYMHmIVJI"
      },
      "source": [
        "| 要素 | 説明 |\n",
        "| ---- | ---- |\n",
        "|id | ID|\n",
        "|odai_photo_file_name | ボケてのお題画像|\n",
        "|text | ボケての文章|\n",
        "|is_laugh | 面白さ（面白い：１、面白くない：０）|"
      ],
      "id": "F0qqYMHmIVJI"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-37D17jzIVg0"
      },
      "source": [
        "ディレクトリ構成は以下のように設定します\n",
        "\n",
        "```\n",
        "├── train.zip\n",
        "│ ├── xxx.jpg\n",
        "│ └── yyy.jpg\n",
        "├── test.zip\n",
        "│ ├── xxx.jpg\n",
        "│ └── yyy.jpg\n",
        "├── train.csv\n",
        "├── test.csv\n",
        "├── sample_submission.csv\n",
        "└── submission.csv(今回のbaselineで生成されるsubmissionファイル)\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "### setting\n",
        "ページ上部の「ランタイム」>「ランタイムのタイプを変更」から「GPU」「ハイメモリ」を選択"
      ],
      "id": "-37D17jzIVg0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0c3Vdk6IR67",
        "outputId": "481be0d6-2394-4d1e-84ee-cd3cf2a2c147"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "p0c3Vdk6IR67"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uj8lSqQzIbCi",
        "outputId": "9572e8e4-f14d-4dde-a268-fdebf86f1c94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Aug 21 01:36:05 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ],
      "id": "uj8lSqQzIbCi"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7Bd4_fTJ7yJ"
      },
      "source": [
        "# Library"
      ],
      "id": "h7Bd4_fTJ7yJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYIZxwL-JUsN",
        "outputId": "0eb30dcd-30cc-4096-d655-221164af8744"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers[ja] in /usr/local/lib/python3.7/dist-packages (4.21.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers[ja]) (0.12.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers[ja]) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers[ja]) (2022.6.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers[ja]) (0.8.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers[ja]) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers[ja]) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers[ja]) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers[ja]) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers[ja]) (4.12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers[ja]) (3.8.0)\n",
            "Requirement already satisfied: unidic-lite>=1.0.7 in /usr/local/lib/python3.7/dist-packages (from transformers[ja]) (1.0.8)\n",
            "Requirement already satisfied: ipadic<2.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from transformers[ja]) (1.0.0)\n",
            "Requirement already satisfied: fugashi>=1.0 in /usr/local/lib/python3.7/dist-packages (from transformers[ja]) (1.1.2)\n",
            "Requirement already satisfied: unidic>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from transformers[ja]) (1.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers[ja]) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers[ja]) (3.0.9)\n",
            "Requirement already satisfied: plac<2.0.0,>=1.1.3 in /usr/local/lib/python3.7/dist-packages (from unidic>=1.0.2->transformers[ja]) (1.3.5)\n",
            "Requirement already satisfied: wasabi<1.0.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from unidic>=1.0.2->transformers[ja]) (0.10.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[ja]) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[ja]) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[ja]) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[ja]) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers[ja]) (3.8.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers[ja]\n",
        "!pip install --quiet sentencepiece"
      ],
      "id": "KYIZxwL-JUsN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94849d2f-b775-4843-ba16-a4e5f6620440"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "import os\n",
        "import random\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import log_loss, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from scipy.special import softmax\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModel, MMBTForClassification, MMBTConfig, AutoConfig,\n",
        "    Trainer, TrainingArguments,\n",
        ")\n",
        "import transformers\n",
        "\n",
        "from torchvision.io import read_image\n",
        "from torchvision.models import ResNet152_Weights, resnet152\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns"
      ],
      "id": "94849d2f-b775-4843-ba16-a4e5f6620440"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "HEHqqmAJNrux",
        "outputId": "798a1464-a987-435d-9ca2-d3b6e329c84f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'4.21.1'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "transformers.__version__"
      ],
      "id": "HEHqqmAJNrux"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbbXbB_rJ49S"
      },
      "source": [
        "# Setting"
      ],
      "id": "UbbXbB_rJ49S"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SFvk38HjNKDb"
      },
      "outputs": [],
      "source": [
        "def seed_everything(seed=1234):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "\n",
        "seed_everything(42)"
      ],
      "id": "SFvk38HjNKDb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "_LogmMSiJ4f5",
        "outputId": "cc09916e-002e-4587-90c8-010670107d4a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n#画像データのzipを解凍します\\n!unzip /content/drive/MyDrive/nishika/train.zip -d /content/drive/MyDrive/nishika/train/\\n!unzip /content/drive/MyDrive/nishika/test.zip -d /content/drive/MyDrive/nishika/test/\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "\"\"\"\n",
        "#画像データのzipを解凍します\n",
        "!unzip /content/drive/MyDrive/nishika/train.zip -d /content/drive/MyDrive/nishika/train/\n",
        "!unzip /content/drive/MyDrive/nishika/test.zip -d /content/drive/MyDrive/nishika/test/\n",
        "\"\"\""
      ],
      "id": "_LogmMSiJ4f5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0a5ed975-d76c-4d6a-8466-71efcfab6099"
      },
      "outputs": [],
      "source": [
        "INPUT = \"/content/drive/MyDrive/nishika/\" # 所望のディレクトリに変更してください。\n",
        "train_image_path = \"/content/drive/MyDrive/nishika/train/\"\n",
        "test_image_path = \"/content/drive/MyDrive/nishika/test/\"\n",
        "\n",
        "train_df = pd.read_csv(os.path.join(INPUT, \"train.csv\"))\n",
        "test_df = pd.read_csv(os.path.join(INPUT, \"test.csv\"))\n",
        "submission_df = pd.read_csv(os.path.join(INPUT, \"sample_submission.csv\"))\n",
        "\n",
        "train_df[\"img_path\"] = train_image_path + train_df[\"odai_photo_file_name\"]\n",
        "test_df[\"img_path\"] = test_image_path + test_df[\"odai_photo_file_name\"]"
      ],
      "id": "0a5ed975-d76c-4d6a-8466-71efcfab6099"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 682
        },
        "id": "wkp7v51-KowI",
        "outputId": "f5471a7a-d891-437b-c6e9-476a9d6bddb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_data: (24962, 5)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "          id odai_photo_file_name  \\\n",
              "0  ge5kssftl       9fkys1gb2r.jpg   \n",
              "1  r7sm6tvkj       c6ag0m1lak.jpg   \n",
              "2  yp5aze0bh       whtn6gb9ww.jpg   \n",
              "3  ujaixzo56       6yk5cwmrsy.jpg   \n",
              "4  7vkeveptl       0i9gsa2jsm.jpg   \n",
              "\n",
              "                                            text  is_laugh  \\\n",
              "0             君しょっちゅうソレ自慢するけど、ツムジ２個ってそんなに嬉しいのかい？         0   \n",
              "1                            これでバレない？授業中寝てもバレない？         0   \n",
              "2  「あなたも感じる？」\\n『ああ…、感じてる…』\\n「後ろに幽霊いるよね…」\\n『女のな…』         0   \n",
              "3                 大塚愛聞いてたらお腹減った…さく、らんぼと牛タン食べたい…          0   \n",
              "4                                    熊だと思ったら嫁だった         0   \n",
              "\n",
              "                                            img_path  \n",
              "0  /content/drive/MyDrive/nishika/train/9fkys1gb2...  \n",
              "1  /content/drive/MyDrive/nishika/train/c6ag0m1la...  \n",
              "2  /content/drive/MyDrive/nishika/train/whtn6gb9w...  \n",
              "3  /content/drive/MyDrive/nishika/train/6yk5cwmrs...  \n",
              "4  /content/drive/MyDrive/nishika/train/0i9gsa2js...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0b581280-a70a-4e88-856c-5db550eac542\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>odai_photo_file_name</th>\n",
              "      <th>text</th>\n",
              "      <th>is_laugh</th>\n",
              "      <th>img_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ge5kssftl</td>\n",
              "      <td>9fkys1gb2r.jpg</td>\n",
              "      <td>君しょっちゅうソレ自慢するけど、ツムジ２個ってそんなに嬉しいのかい？</td>\n",
              "      <td>0</td>\n",
              "      <td>/content/drive/MyDrive/nishika/train/9fkys1gb2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>r7sm6tvkj</td>\n",
              "      <td>c6ag0m1lak.jpg</td>\n",
              "      <td>これでバレない？授業中寝てもバレない？</td>\n",
              "      <td>0</td>\n",
              "      <td>/content/drive/MyDrive/nishika/train/c6ag0m1la...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>yp5aze0bh</td>\n",
              "      <td>whtn6gb9ww.jpg</td>\n",
              "      <td>「あなたも感じる？」\\n『ああ…、感じてる…』\\n「後ろに幽霊いるよね…」\\n『女のな…』</td>\n",
              "      <td>0</td>\n",
              "      <td>/content/drive/MyDrive/nishika/train/whtn6gb9w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ujaixzo56</td>\n",
              "      <td>6yk5cwmrsy.jpg</td>\n",
              "      <td>大塚愛聞いてたらお腹減った…さく、らんぼと牛タン食べたい…</td>\n",
              "      <td>0</td>\n",
              "      <td>/content/drive/MyDrive/nishika/train/6yk5cwmrs...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7vkeveptl</td>\n",
              "      <td>0i9gsa2jsm.jpg</td>\n",
              "      <td>熊だと思ったら嫁だった</td>\n",
              "      <td>0</td>\n",
              "      <td>/content/drive/MyDrive/nishika/train/0i9gsa2js...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0b581280-a70a-4e88-856c-5db550eac542')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0b581280-a70a-4e88-856c-5db550eac542 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0b581280-a70a-4e88-856c-5db550eac542');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_data: (6000, 4)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "          id odai_photo_file_name  \\\n",
              "0  rfdjcfsqq       nc1kez326b.jpg   \n",
              "1  tsgqmfpef       49xt2fmjw0.jpg   \n",
              "2  owjcthkz2       9dtscjmyfh.jpg   \n",
              "3  rvgaocjyy       osa3n56tiv.jpg   \n",
              "4  uxtwu5i69       yb1yqs4pvb.jpg   \n",
              "\n",
              "                                                text  \\\n",
              "0                          僕のママ、キャラ弁のゆでたまごに８時間かかったんだ   \n",
              "1                                          かわいいが作れた！   \n",
              "2                                           来世の志茂田景樹   \n",
              "3                       ちょ、あの、オカン、これ水風呂やねんけど、なんの冗談??   \n",
              "4  「今日は皆さんにザリガニと消防車の違いを知ってもらいたいと思います」『どっちも同じだろ。両方...   \n",
              "\n",
              "                                            img_path  \n",
              "0  /content/drive/MyhDrive/nishika/test/nc1kez326...  \n",
              "1  /content/drive/MyhDrive/nishika/test/49xt2fmjw...  \n",
              "2  /content/drive/MyhDrive/nishika/test/9dtscjmyf...  \n",
              "3  /content/drive/MyhDrive/nishika/test/osa3n56ti...  \n",
              "4  /content/drive/MyhDrive/nishika/test/yb1yqs4pv...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-11fcc957-0cc3-4f1c-8882-88f9cc433844\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>odai_photo_file_name</th>\n",
              "      <th>text</th>\n",
              "      <th>img_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>rfdjcfsqq</td>\n",
              "      <td>nc1kez326b.jpg</td>\n",
              "      <td>僕のママ、キャラ弁のゆでたまごに８時間かかったんだ</td>\n",
              "      <td>/content/drive/MyhDrive/nishika/test/nc1kez326...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>tsgqmfpef</td>\n",
              "      <td>49xt2fmjw0.jpg</td>\n",
              "      <td>かわいいが作れた！</td>\n",
              "      <td>/content/drive/MyhDrive/nishika/test/49xt2fmjw...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>owjcthkz2</td>\n",
              "      <td>9dtscjmyfh.jpg</td>\n",
              "      <td>来世の志茂田景樹</td>\n",
              "      <td>/content/drive/MyhDrive/nishika/test/9dtscjmyf...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>rvgaocjyy</td>\n",
              "      <td>osa3n56tiv.jpg</td>\n",
              "      <td>ちょ、あの、オカン、これ水風呂やねんけど、なんの冗談??</td>\n",
              "      <td>/content/drive/MyhDrive/nishika/test/osa3n56ti...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>uxtwu5i69</td>\n",
              "      <td>yb1yqs4pvb.jpg</td>\n",
              "      <td>「今日は皆さんにザリガニと消防車の違いを知ってもらいたいと思います」『どっちも同じだろ。両方...</td>\n",
              "      <td>/content/drive/MyhDrive/nishika/test/yb1yqs4pv...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-11fcc957-0cc3-4f1c-8882-88f9cc433844')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-11fcc957-0cc3-4f1c-8882-88f9cc433844 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-11fcc957-0cc3-4f1c-8882-88f9cc433844');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "print(f\"train_data: {train_df.shape}\")\n",
        "display(train_df.head())\n",
        "\n",
        "print(f\"test_data: {test_df.shape}\")\n",
        "display(test_df.head())"
      ],
      "id": "wkp7v51-KowI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ae97fdbf"
      },
      "outputs": [],
      "source": [
        "test_df[\"is_laugh\"] = 0"
      ],
      "id": "ae97fdbf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71lg9MEu0O5X"
      },
      "source": [
        "# MMBT\n",
        "MMBTとはMultiModal BiTransformersの略であり、BERTをベースとした画像とテキストのマルチモーダルディープラーニングです。画像にはResNet152を、テキスト側はBERTを用いてそれぞれベクトル変換し、両方をtokenとして連結したものに再度BERTに入力します。  \n",
        "https://arxiv.org/pdf/1909.02950.pdf\n",
        "\n",
        "https://github.com/facebookresearch/mmbt\n",
        "\n",
        "すでにhuggingface内にモデルがあるので、今回はこちらを使用していきたいと思います。\n",
        "https://huggingface.co/docs/transformers/main/en/model_summary#multimodal-models\n"
      ],
      "id": "71lg9MEu0O5X"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c564cb33-70f3-4674-afc3-d59115442e74"
      },
      "outputs": [],
      "source": [
        "# 画像データをEmbeddingしていきます\n",
        "class ImageEncoder(nn.Module):\n",
        "    POOLING_BREAKDOWN = {1: (1, 1), 2: (2, 1), 3: (3, 1), 4: (2, 2), 5: (5, 1), 6: (3, 2), 7: (7, 1), 8: (4, 2), 9: (3, 3)}\n",
        "    def __init__(self, pretrained_weight):\n",
        "        super().__init__()\n",
        "        model = resnet152(weights=pretrained_weight)\n",
        "        modules = list(model.children())[:-2]\n",
        "        self.model = nn.Sequential(*modules)\n",
        "        self.pool = nn.AdaptiveAvgPool2d(self.POOLING_BREAKDOWN[3])\n",
        "\n",
        "    def forward(self,  x):\n",
        "        out = self.pool(self.model(x))\n",
        "        out = torch.flatten(out, start_dim=2)\n",
        "        out = out.transpose(1, 2).contiguous()\n",
        "        return out"
      ],
      "id": "c564cb33-70f3-4674-afc3-d59115442e74"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cffeb040"
      },
      "outputs": [],
      "source": [
        "def read_jpg(path):\n",
        "    image_tensor = read_image(path)\n",
        "    if image_tensor.shape[0] == 1:\n",
        "        # 1channel=白黒画像があるので3channelにconvertしています。\n",
        "        image_tensor = image_tensor.expand(3, *image_tensor.shape[1:])\n",
        "    return image_tensor\n",
        "\n",
        "class BoketeTextImageDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, max_seq_len:int, image_transform):\n",
        "        self.df = df\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.image_transforms = image_transform.transforms()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        row = self.df.iloc[index]\n",
        "        sentence = torch.tensor(self.tokenizer.encode(row[\"text\"], max_length=self.max_seq_len, padding=\"max_length\", truncation=True))\n",
        "        start_token, sentence, end_token = sentence[0], sentence[1:-1], sentence[-1]\n",
        "        sentence = sentence[:self.max_seq_len]\n",
        "\n",
        "        image = self.image_transforms(read_jpg(row[\"img_path\"]))\n",
        "\n",
        "        return {\n",
        "            \"image_start_token\": start_token,\n",
        "            \"image_end_token\": end_token,\n",
        "            \"sentence\": sentence,\n",
        "            \"image\": image,\n",
        "            \"label\": torch.tensor(row[\"is_laugh\"]),\n",
        "        }\n",
        "\n",
        "def collate_fn(batch):\n",
        "    lens = [len(row[\"sentence\"]) for row in batch]\n",
        "    bsz, max_seq_len = len(batch), max(lens)\n",
        "\n",
        "    mask_tensor = torch.zeros(bsz, max_seq_len, dtype=torch.long)\n",
        "    text_tensor = torch.zeros(bsz, max_seq_len, dtype=torch.long)\n",
        "\n",
        "    for i_batch, (input_row, length) in enumerate(zip(batch, lens)):\n",
        "        text_tensor[i_batch, :length] = input_row[\"sentence\"]\n",
        "        mask_tensor[i_batch, :length] = 1\n",
        "\n",
        "    img_tensor = torch.stack([row[\"image\"] for row in batch])\n",
        "    tgt_tensor = torch.stack([row[\"label\"] for row in batch])\n",
        "    img_start_token = torch.stack([row[\"image_start_token\"] for row in batch])\n",
        "    img_end_token = torch.stack([row[\"image_end_token\"] for row in batch])\n",
        "\n",
        "    return {\n",
        "        \"input_ids\":text_tensor,\n",
        "        \"attention_mask\":mask_tensor,\n",
        "        \"input_modal\":img_tensor,\n",
        "        \"modal_start_tokens\":img_start_token,\n",
        "        \"modal_end_tokens\":img_end_token,\n",
        "        \"labels\":tgt_tensor,\n",
        "    }"
      ],
      "id": "cffeb040"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQVb4n2G4Cub"
      },
      "source": [
        "学習済みモデルには、東北大学の乾研究室が作成したものを使用します。"
      ],
      "id": "gQVb4n2G4Cub"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cefa6c4"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese-whole-word-masking\")"
      ],
      "id": "6cefa6c4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtLhjLp_GWti"
      },
      "source": [
        "# Data Split"
      ],
      "id": "WtLhjLp_GWti"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1098e98"
      },
      "outputs": [],
      "source": [
        "trn_idx, val_idx = train_test_split(list(range(len(train_df))), test_size=0.2, random_state=42, stratify=train_df[\"is_laugh\"])"
      ],
      "id": "a1098e98"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8028135"
      },
      "outputs": [],
      "source": [
        "trn_ds = BoketeTextImageDataset(train_df.iloc[trn_idx], tokenizer, 48, image_transform=ResNet152_Weights.IMAGENET1K_V2)\n",
        "val_ds = BoketeTextImageDataset(train_df.iloc[val_idx], tokenizer, 48, image_transform=ResNet152_Weights.IMAGENET1K_V2)"
      ],
      "id": "d8028135"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5db01c7"
      },
      "outputs": [],
      "source": [
        "test_ds = BoketeTextImageDataset(test_df, tokenizer, 48, image_transform=ResNet152_Weights.IMAGENET1K_V2)"
      ],
      "id": "d5db01c7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6dd4362",
        "outputId": "36d988c1-cf53-432a-edb0-a7dc2c01075c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "transformer_config = AutoConfig.from_pretrained(\"cl-tohoku/bert-base-japanese-whole-word-masking\")\n",
        "transformer = AutoModel.from_pretrained(\"cl-tohoku/bert-base-japanese-whole-word-masking\")"
      ],
      "id": "d6dd4362"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4d3dd69d"
      },
      "outputs": [],
      "source": [
        "config = MMBTConfig(transformer_config, num_labels=2)\n",
        "model = MMBTForClassification(config, transformer, ImageEncoder(ResNet152_Weights.IMAGENET1K_V2))"
      ],
      "id": "4d3dd69d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aafd6ea9"
      },
      "outputs": [],
      "source": [
        "config.use_return_dict = True"
      ],
      "id": "aafd6ea9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5a3873df"
      },
      "outputs": [],
      "source": [
        "model.config = model.mmbt.config"
      ],
      "id": "5a3873df"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7d9e256a"
      },
      "outputs": [],
      "source": [
        "trainer_args = TrainingArguments(\n",
        "    output_dir=\"/content/MyDrive/nishika/mmbt_exp01\",\n",
        "    overwrite_output_dir=True,\n",
        "    do_train=True,\n",
        "    do_eval=True,\n",
        "    num_train_epochs=3,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=50,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=50,\n",
        "    logging_steps=50,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=12,\n",
        "    save_total_limit=1,\n",
        "    fp16=True,\n",
        "    remove_unused_columns=False,\n",
        "    gradient_accumulation_steps=20,\n",
        "    load_best_model_at_end=True,\n",
        "    logging_dir='./logs',\n",
        "    report_to=\"none\"\n",
        ")"
      ],
      "id": "7d9e256a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ef9b39c3",
        "outputId": "a7b98379-e3a6-4de9-aa51-a457e8cf6bec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cuda_amp half precision backend\n"
          ]
        }
      ],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=trainer_args,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=trn_ds,\n",
        "    eval_dataset=val_ds,\n",
        "    data_collator=collate_fn,\n",
        ")"
      ],
      "id": "ef9b39c3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuVZt4k6FvcP"
      },
      "source": [
        "実際に学習を進めていきます。"
      ],
      "id": "uuVZt4k6FvcP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "95521a7e",
        "outputId": "ba5b08b1-b9de-4307-e307-357471e91328"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 19969\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 160\n",
            "  Gradient Accumulation steps = 20\n",
            "  Total optimization steps = 372\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='372' max='372' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [372/372 37:18, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.695800</td>\n",
              "      <td>0.694869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.674200</td>\n",
              "      <td>0.665384</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.675400</td>\n",
              "      <td>0.661036</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.661200</td>\n",
              "      <td>0.665689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.680300</td>\n",
              "      <td>0.663123</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.654300</td>\n",
              "      <td>0.657202</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.644000</td>\n",
              "      <td>0.654507</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 4993\n",
            "  Batch size = 12\n",
            "Saving model checkpoint to /content/MyDrive/nishika/mmbt_exp01/checkpoint-50\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "tokenizer config file saved in /content/MyDrive/nishika/mmbt_exp01/checkpoint-50/tokenizer_config.json\n",
            "Special tokens file saved in /content/MyDrive/nishika/mmbt_exp01/checkpoint-50/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4993\n",
            "  Batch size = 12\n",
            "Saving model checkpoint to /content/MyDrive/nishika/mmbt_exp01/checkpoint-100\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "tokenizer config file saved in /content/MyDrive/nishika/mmbt_exp01/checkpoint-100/tokenizer_config.json\n",
            "Special tokens file saved in /content/MyDrive/nishika/mmbt_exp01/checkpoint-100/special_tokens_map.json\n",
            "Deleting older checkpoint [/content/MyDrive/nishika/mmbt_exp01/checkpoint-50] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4993\n",
            "  Batch size = 12\n",
            "Saving model checkpoint to /content/MyDrive/nishika/mmbt_exp01/checkpoint-150\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "tokenizer config file saved in /content/MyDrive/nishika/mmbt_exp01/checkpoint-150/tokenizer_config.json\n",
            "Special tokens file saved in /content/MyDrive/nishika/mmbt_exp01/checkpoint-150/special_tokens_map.json\n",
            "Deleting older checkpoint [/content/MyDrive/nishika/mmbt_exp01/checkpoint-100] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4993\n",
            "  Batch size = 12\n",
            "Saving model checkpoint to /content/MyDrive/nishika/mmbt_exp01/checkpoint-200\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "tokenizer config file saved in /content/MyDrive/nishika/mmbt_exp01/checkpoint-200/tokenizer_config.json\n",
            "Special tokens file saved in /content/MyDrive/nishika/mmbt_exp01/checkpoint-200/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4993\n",
            "  Batch size = 12\n",
            "Saving model checkpoint to /content/MyDrive/nishika/mmbt_exp01/checkpoint-250\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "tokenizer config file saved in /content/MyDrive/nishika/mmbt_exp01/checkpoint-250/tokenizer_config.json\n",
            "Special tokens file saved in /content/MyDrive/nishika/mmbt_exp01/checkpoint-250/special_tokens_map.json\n",
            "Deleting older checkpoint [/content/MyDrive/nishika/mmbt_exp01/checkpoint-200] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4993\n",
            "  Batch size = 12\n",
            "Saving model checkpoint to /content/MyDrive/nishika/mmbt_exp01/checkpoint-300\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "tokenizer config file saved in /content/MyDrive/nishika/mmbt_exp01/checkpoint-300/tokenizer_config.json\n",
            "Special tokens file saved in /content/MyDrive/nishika/mmbt_exp01/checkpoint-300/special_tokens_map.json\n",
            "Deleting older checkpoint [/content/MyDrive/nishika/mmbt_exp01/checkpoint-150] due to args.save_total_limit\n",
            "Deleting older checkpoint [/content/MyDrive/nishika/mmbt_exp01/checkpoint-250] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4993\n",
            "  Batch size = 12\n",
            "Saving model checkpoint to /content/MyDrive/nishika/mmbt_exp01/checkpoint-350\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "tokenizer config file saved in /content/MyDrive/nishika/mmbt_exp01/checkpoint-350/tokenizer_config.json\n",
            "Special tokens file saved in /content/MyDrive/nishika/mmbt_exp01/checkpoint-350/special_tokens_map.json\n",
            "Deleting older checkpoint [/content/MyDrive/nishika/mmbt_exp01/checkpoint-300] due to args.save_total_limit\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from /content/MyDrive/nishika/mmbt_exp01/checkpoint-350 (score: 0.6545068621635437).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=372, training_loss=0.6669345773676391, metrics={'train_runtime': 2255.8411, 'train_samples_per_second': 26.556, 'train_steps_per_second': 0.165, 'total_flos': 0.0, 'train_loss': 0.6669345773676391, 'epoch': 2.99})"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "trainer.train()"
      ],
      "id": "95521a7e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "2eed4ed7",
        "outputId": "62189f0e-ee94-4095-ae1f-38af441b6ffc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Prediction *****\n",
            "  Num examples = 4993\n",
            "  Batch size = 12\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              ""
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "val_preds = trainer.predict(val_ds).predictions"
      ],
      "id": "2eed4ed7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ba6b6063",
        "outputId": "09ae7f2a-fb14-4ff4-bd52-9debf2543e75"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6544280034039843"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "# sanity check\n",
        "log_loss(val_ds.df[\"is_laugh\"].values, softmax(val_preds, axis=-1))"
      ],
      "id": "ba6b6063"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a088d18d",
        "outputId": "a16e5dc7-9f0a-403c-fdb3-8199e749fe14"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6226717404366112"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "accuracy_score(val_ds.df[\"is_laugh\"].values, np.argmax(val_preds, axis=-1))"
      ],
      "id": "a088d18d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "id": "1t2NSipaF3Nm",
        "outputId": "04cb0786-a764-4c27-edec-d9baa828dcd6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 51.0, 'Predict')"
            ]
          },
          "metadata": {},
          "execution_count": 28
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x576 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "_conf_options = {\"normalize\": None,}\n",
        "_plot_options = {\n",
        "        \"cmap\": \"Blues\",\n",
        "        \"annot\": True\n",
        "    }\n",
        "\n",
        "conf = confusion_matrix(y_true=val_ds.df[\"is_laugh\"].values,\n",
        "                        y_pred=np.argmax(val_preds, axis=-1),\n",
        "                        **_conf_options)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "sns.heatmap(conf, ax=ax, **_plot_options)\n",
        "ax.set_ylabel(\"Label\")\n",
        "ax.set_xlabel(\"Predict\")"
      ],
      "id": "1t2NSipaF3Nm"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vpkqkt9WF1Y7"
      },
      "source": [
        "# Predict"
      ],
      "id": "Vpkqkt9WF1Y7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "8cb6383d",
        "outputId": "a4fceb3a-98c2-4ded-ba54-3c6e55d5c6eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Prediction *****\n",
            "  Num examples = 6000\n",
            "  Batch size = 12\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              ""
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "preds = trainer.predict(test_ds).predictions"
      ],
      "id": "8cb6383d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5b90a87"
      },
      "outputs": [],
      "source": [
        "submission_df[\"is_laugh\"] = softmax(preds, axis=-1)[:, 1]"
      ],
      "id": "e5b90a87"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00c15ee4"
      },
      "outputs": [],
      "source": [
        "submission_df[\"is_laugh\"] = submission_df[\"is_laugh\"].astype(float)"
      ],
      "id": "00c15ee4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a74a2b0d"
      },
      "outputs": [],
      "source": [
        "##OUTPUT = \"/content/drive/MyDrive/nishika/sub\" # ディレクトリを指定してください\n",
        "submission_df.to_csv(os.path.join('sub.csv'), index=False)"
      ],
      "id": "a74a2b0d"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.12 ('.venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "cba46f5f40656f25ffb6c872a20afb3d6656178238b29214063df46797159f70"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
